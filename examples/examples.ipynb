{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gzeus import Chunker, stream_polars_csv_gz\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The dataset is available at:\n",
    "# https://catalog.data.gov/dataset/insurance-complaints-all-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_load_data_pandas(path:str, chunk_size:int = 10_000) -> pd.DataFrame:\n",
    "    frames = [\n",
    "        df_chunk\n",
    "        for df_chunk in pd.read_csv(\"../data/insurance.csv.gz\", iterator = True, chunksize=chunk_size, engine=\"c\")   \n",
    "    ]\n",
    "    return pd.concat(frames)\n",
    "\n",
    "def chunk_load_data_gzeus2(path:str, chunk_size:int = 1_000_000) -> pl.DataFrame:\n",
    "    # Turn portion of the produced bytes into a DataFrame. Only possible with Polars, \n",
    "    # or dataframe packages with \"lazy\" capabilities. Lazy read + filters ensure \n",
    "    # only necessary bytes are copied into our dataframe \n",
    "    def get_necessary_data(df:pl.LazyFrame) -> pl.DataFrame:\n",
    "        return df.filter(\n",
    "            (pl.col(\"Confirmed complaint\") != 'No')\n",
    "            & (pl.col(\"Keywords\").is_not_null())\n",
    "        ).select(\n",
    "            'Complaint number',\n",
    "            'Complaint filed against',\n",
    "            'Complaint filed by',\n",
    "            'Reason complaint filed',\n",
    "            'Confirmed complaint',\n",
    "            'Received date',\n",
    "            'Closed date',\n",
    "            'Complaint type',\n",
    "            'Coverage level',\n",
    "        ).collect()\n",
    "\n",
    "    return pl.concat(stream_polars_csv_gz(path, buffer_size=chunk_size, func = get_necessary_data))\n",
    "\n",
    "def chunk_load_data_pandas2(path:str, chunk_size:int = 10_000) -> pd.DataFrame:\n",
    "    def get_necessary_data(df:pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.loc[\n",
    "            (df['Confirmed complaint'] != 'No') & (~df['Keywords'].isna())\n",
    "            , :\n",
    "        ][[\n",
    "            'Complaint number',\n",
    "            'Complaint filed against',\n",
    "            'Complaint filed by',\n",
    "            'Reason complaint filed',\n",
    "            'Confirmed complaint',\n",
    "            'Received date',\n",
    "            'Closed date',\n",
    "            'Complaint type',\n",
    "            'Coverage level',\n",
    "        ]]\n",
    "\n",
    "    frames = [\n",
    "        get_necessary_data(df_chunk)\n",
    "        for df_chunk in pd.read_csv(\"../data/insurance.csv.gz\", iterator = True, chunksize=chunk_size, engine=\"c\")   \n",
    "    ]\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want more logging info, set verbose=True\n",
    "# and set up your own logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.concat(stream_polars_csv_gz(\"../data/insurance.csv.gz\", verbose=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = chunk_load_data_pandas(\"../data/insurance.csv.gz\")\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 1 - (GZeus + Polars) vs. Pandas \n",
    "\n",
    "No work per chunk.\n",
    "\n",
    "Tuning pandas chunksize doesn't really help with performance at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = pl.concat(stream_polars_csv_gz(\"../data/insurance.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas(\"../data/insurance.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = pl.concat(stream_polars_csv_gz(\"../data/insurance.csv.gz\", buffer_size=5_000_000)) # bigger chunks, 5mb per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas(\"../data/insurance.csv.gz\", chunk_size=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas(\"../data/insurance.csv.gz\", chunk_size=100_000) # the whole df is 260k rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 2 - (GZeus + Polars) vs. Pandas \n",
    "\n",
    "Some work per chunk.\n",
    "\n",
    "GZeus + Polars runs faster because the workload can be optimized by Polars and further speeds up the process. On the other hand, pandas shows no speed improvement because it will read the full chunk regardless of any work you do on the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_gzeus2(\"../data/insurance.csv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas2(\"../data/insurance.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_gzeus2(\"../data/insurance.csv.gz\", chunk_size=5_000_000) # bigger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas2(\"../data/insurance.csv.gz\", chunk_size=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = chunk_load_data_pandas2(\"../data/insurance.csv.gz\", chunk_size=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
